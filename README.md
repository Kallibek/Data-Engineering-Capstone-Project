# Udacity Data Engineering Nanodegree Capstone Project
Topic: Data Engineering for Vehicle Collisions and Weather Analytics.

by Kallibek Kazbekov

Date: 4/8/2021

---
# Project summary

## Overview

The code in this project builds a scheduled data engineering pipeline to prepare data for both descriptive and predictive analytics. An example of descriptive analytics could be a dashboard showing a count of accidents that happened yesterday. In the case of predictive analytics, extracted-transformed-loaded data might be used to understand the relationship between weather parameters and vehicle collisions to be able to prepare for severe weather events and associated road accidents. 

New York City was picked as a study. 

Tools used: Airflow, PySpark, S3.

## Data Collection and ETL

The data collection and ETL are orchestrated by Airflow that triggers the code execution every day at 7 am. ETL steps are coded in Jupyter notebook and run on an EMR cluster. 
At every DAG execution, final results (tables) are loaded into the project S3 bucket.

![Alt text](airflow_dag.png?raw=false "Airflow DAG")

## Feasibility under different scenarios

#### The data was increased by 100x.

This scenario is highly possible. However, S3-based storage will be still valid under this possibility. In the case of processing power, EMR can be easily scaled both horizontally and vertically to meet the data processing needs. Since the project addresses OLAP-related questions, the code is run only once a day and processing are not heavy neither in reading nor writing.

#### The pipelines would be run on a daily basis by 7 am every day

The ETL implementation is already scheduled to run daily at 7 am using Airflow. If the DAG execution fails, the data collection part can be done by manually running bash scripts used in Bash Operator tasks on any other machine. The ETL part which is a Jupyter-notebook-based operation can simply be transferred  to any other Spark Cluster and run manually. 

#### The database needed to be accessed by 100+ people

The results of ETL (tables) are immediately loaded to S3 as parquet and CSV files and can be accessed without any downsides by 100+ people. However, the ETL is not for OLTP operation, and therefore it is least possible that 100+ people need simultaneous access to data.  

## Justification of used tools and technologies

* Airflow. It is a free, open-source tool that has a huge community of active users and contributors. Also, it allows to intuitively build a sequence of tasks using Python. 
* AWS CLI and Boto3. They are required to access AWS services programmatically.
* EMR. A significant benefit of using an EMR cluster is that a Jupyter notebook can be submitted as a spark job instead of .jar and .py files.
* S3. It is scalable, cost-effective, reliable, and durable in addition to its well-developed API to interact programmatically with it. 

## Justification of data model 

The generated tables are unnormalized (except for Time table) and some are joined before final uploading to S3 in order to achieve high query speed during further analytics. Specifically, the original weather data's format was only reshaped (untidy to tidy). There is no need to further normalize weather data because it has only a date and five columns each of which can be identified using date column independently from other columns. 

Time table is a dimensional table that was exported using datetime columns of the Collisions dataset. This schema allows to easily join collisions (fact table) with time table (dimensional) to answer time-related queries like "What time of a day or which month is associated with the highest number of collisions?" and etc. 

The Collision table was summarized by date because the daily summary is the most frequent query run by Data Analysts.

The reason for pre-joining daily collisions summary with daily weather data is to skip the unavoidable SQL join to find the statistical relationship between weather and collision.

## Data Dictionary

* EMR - Elastic Map Reduce
* S3 - Simple Storage Service
* AWS CLI -  AWS Command Line Interface
* Boto3 - Python SDK for AWS
* DAG - Directed acyclic graph 

#### NYC Vehicle Collisions dataset

| Column Name | Description | Type |
|-------------|-------------|------|
|NYC Vehicle Collisions dataset|
|Collision ID|Unique record code generated by system. Primary Key for Crash table.|Number|
|Datetime|Occurrence datetime of collision|Datetime|
|Zipcode|Postal code of incident occurrence|Text|
|Coordinates|occurrence latitude , longitude pair |Text|
|Injured|Number of persons injured|Number|
|Killed|Number of persons killed|Number|
|Year|Year of occurrence. Extracted from datetime to partition later on|Number|
|Month|Month of occurrence. Extracted from datetime to partition later on|Number|

#### NYC Vehicle Collisions Daily Summary

| Column Name | Description | Type |
|-------------|-------------|------|
|Date|Date of interest|Date|
|Collisions|Count of collisions in a day|Number|
|Injured|Total number of persons injured in a day|Number|
|Killed|Total number of persons killed in a day|Number|

#### Daily Weather table

| Column Name | Description | Type |
|-------------|-------------|------|
|Date|Date|Date|
|PRCP|Precipitation (tenth of mm)|Number|
|SNOW|Snowfall (mm)|Number|
|SNWD|Snow depth (mm)|Number|
|TMAX|Maximum temperature (C)|Number|
|TMIN|Minimum temperature (C)|Number|

# Data Collection

Weather data comes from Daily Global Historical Climatology Network (GHCN-DAILY) Version 3.26  (ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily). 

The second dataset is The Motor Vehicle Collisions which contains information from all police reported motor vehicle collisions in NYC (https://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Crashes/h9gi-nx95).

Both datasets are updated on daily basis.

Data Collection consists of three stage:

1. Download datasets from original sources;
2. Upload to S3 project bucket;
3. Clean downloaded data from local machine.


# ETL on Spark cluster

All ETL processes are implemented in a jupyter notebook (spark_etl.ipynb). After data collection is complete Airflow submits a notebook execution task to a running EMR cluster using a python operator executing a boto3 command. While running the Python Operator logs status updates every 30 seconds.

## Step 1: Load data from S3

In this step, both datasets are extracted from S3 bucket to Spark. Also, data corresponding to the Central Park Weather Station (ID USW00094728) is filtered. 

## Step 2: Transform data

Here, Spark:

1. summarizes collisions table by date, summarize by count of collisions, sum of injured and killed;
2. makes a separate table called Time table by extracting datetime from collisions table and also extracting elements like year, day, hour from the datetime;
3. reshapes weather data to be in a tidy format.

![Alt text](relational_diagram.png?raw=false "Data Model")


## Step 3: Join dataframes for analytics

During this step, daily summaries of collisions and daily weather data are inner-joined to be able to analyze patterns between weather and collisions.

## Step 4: Load to S3

Finally, the Spark writes generated and transformed dataframes to parquet and csv files in S3 bucket. 

![Alt text](data_collection_and_etl.png?raw=false "Data collection and ETL")

# Project instructions on how to run the Python scripts

The project uses AWS CLI v2 and Boto3 v1.17.

# An explanation of the files in the repository

## `airflow/dags/dag.py`

The file defines the DAG.

`dag = DAG('dag_id', default_args=default_args,...)` - DAG initialization

```python
def emr_execute_notebook(emr_client,
                         EditorId,
                         RelativePath,
                         cluster_id,
                         ServiceRole):
    """
    Executes notebook on EMR cluster.
    Logs execution status every 30 seconds.
    
    Input:
        emr_client - boto3 EMR client with AWS credentials;
        EditorId - Notebook Id;
        RelativePath - relative path to a notebook;
        cluster_id - ID of a running cluster;
        ServiceRole - service role name to execute a notebook.
    """
```

## `./spark_etl.ipynb` - contains code for all ETL steps.

Each dataframe in the ETL process is checked for quality (count of rows).

```python
def quality_check(df):
    """
    Prints five rows of a dataframe.
    Then, checks count of rows.
    If rows<1 raises an exception
    
    Input: df - pyspark DataFrame
    """
    row_count=df.count()
    print(df.limit(5).toPandas())
    if row_count<1:
        raise Exception("Datafrane has no rows")
    print('count of rows: ', row_count)    
```

## `aws.cfg`

The file contains AWS credentials (access key ID  and secret access key) and other details 
for programmatic access to AWS services.

[Credentials]
aws_access_key_id=''
aws_secret_access_key=''

[default]
region=''

[S3]
project_bucket = ''

[EMR]
cluster_id=''
notebook_id=''
notebookRelativePath=''
